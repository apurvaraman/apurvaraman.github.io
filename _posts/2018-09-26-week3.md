---
layout: post
title: Sorting through thoughts on data, objectivity, and interpretation
---

If we accept data not as empirical, but as interpreted or representations, we have to consider how that interpretation is done. Data gives us an approximation or selection of reports of events. Perhaps data is indexical to events or the world.

Rather than stopping us from doing anything because we aren’t assuming data gives us some objective Truth, this empowers us to have ways to evaluate the data. We can ask questions about data collection methodology and types of data and reporting and evaluate them and compare them. To examine how this evaluation or comparison process works, we might be able to take from the ‘shirtsleeves’ metaphor for objectivity. Similarly to objectivity, perhaps data collection methods and standards around them are created based on what works- what, through previous iterations, has results that are desirable. We can then choose to trust a dataset based on criteria that people who have worked with data deem useful and based on the situation and needs we are collecting the data for.

Data collected with one methodology might be trustworthy in one context and not trustworthy in another. To take an example relevant to the work I’m currently doing in ADE, survey data that was collected by having students self-report mastery of optics after being taught with a certain curriculum might be “trustworthy” for figuring out student engagement and confidence levels, but would not necessarily be “trustworthy” to evaluate whether students come out of the class knowing optics. A standard rule seems difficult to identify and the closest thing I can imagine to a universal standard in data practices now is that of utility.

In *Objectivity*, Daston and Galison discuss epistemic virtues (I’m thinking of this as justified/justifiable grounds for belief) of “truth to nature,” “mechanical objectivity,” and “trained judgement.” In their analysis of the interplay between these three things, they discuss that this is a continuous mingling and reframing; “this is not some neat Hegelian arithmetic of thesis plus antithesis equals synthesis, but a far messier situation in which all the elements continue in play and in interaction with one another.” My initial reaction was surprise; I don’t know that I could easily call Hegelian dialectic “neat”. Nevertheless, as I thought about it more, there is still that underlying assumption of a Truth independent of external determination implicit in such a dialectic process, which is subverted and made messy by introducing concepts of the “para-empirical”/ truthy/ objective enough. (Maybe the process of Hegelian thought is complex but the resulting truth is neat? Maybe I need to actually sit down and read more Hegel.) I would like to re-emphasize the point that even the absence of Truth doesn’t stop us from doing science or collecting, evaluating and acting on data. The example of Latour helped because I find myself going back to Latour again and again when thinking about this sort of thing. Identifying the (social) construction of scientific facts isn’t something that devalues science. It instead puts it in a place where its methods can be challenged and evaluated, and when results continue to hold up to that kind of scrutiny, it makes them stronger (in my opinion!).  

We can return to those epistemic virtues on a more granular, concrete level. Data sets can reflect events, which could be one kind of truth. They could reflect those who collected them or the methods in which they were collected. Data could also reflect the societal conditions that would lead to the prioritization of collecting this data. I suspect that you could gleam a bit of each one of these things from any well-documented, labeled dataset.
